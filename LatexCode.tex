\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[]{algorithm2e}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Sentiment Analysis and Extractive Summarization of Amazon Review Dataset\\
{\footnotesize}
}

\author{\IEEEauthorblockN{Dr. Rajendra Roul}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{BITS Pilani, K.K.Birla Goa Campus}\\
}
\and
\IEEEauthorblockN{Aditya Agarwal}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{BITS Pilani, K.K.Birla Goa Campus}\\
}
\and
\IEEEauthorblockN{Akash Gupta}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{BITS Pilani, K.K.Birla Goa Campus}\\
}
\and
\IEEEauthorblockN{Atul Shanbhag}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{BITS Pilani, K.K.Birla Goa Campus}\\
}
\and
\IEEEauthorblockN{Kathan Kashiparekh}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{BITS Pilani, K.K.Birla Goa Campus}\\
}
\and
\IEEEauthorblockN{Saurabh Shekhar}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{BITS Pilani, K.K.Birla Goa Campus}\\
}
}


\maketitle

\begin{abstract}
With the increase in presence of online shopping companies like Amazon, E-Bay and
Flipkart, people give a wide range of reviews for the products they purchase. Some are too
long, some too short, some difficult to understand while some are totally irrelevant. Thus,
there is a pressing need to reduce the diversity of reviews for a particular product and show
the users only the most useful and most important of reviews about a product. In this work,
we aim to summarize the reviews for movies purchased from Amazon using a combination
of already developed algorithms and a feature selection technique. Sentiment analysis has been
performed to categorize reviews into positive and negative. We have used a novel method for extractive
summarization known as hierarchical summarization to summarize large reviews into summaries having
few number of sentences. The results of this summary are compared to the existing algorithms
using the ROUGE score to determine the best summary.
\end{abstract}

\section{Introduction}
With an ever increasing trend of online shopping, people tend to rate their purchases by
giving reviews of a product on the e-commerce companies websites. These reviews are
seldom informative along and are quite large in number. Thus there is a need to combine
these reviews and represent them in a short summary by extracting the important essence
of the reviews. This technique of summarizing documents into short summaries is known
as Text Summarization.
Traditionally, Text Summarization algorithms are classified into two categories. Extractive
Summarization is where the sentences in a document are ranked based on their relative
importance in the document and the top such sentences chosen. Abstractive summarization
on the other hand summarizes the docoument in its own words analogus to how we
summarize some event to our friend in our own words. In order to achieve this,
Abstractive summarization requires lots of data and complex algorithms thereby making it
a difficult task to achieve.\\
However, in order to judge the accuracy of the generated summaries, we have “human-
generated summaries” which are summaries generated by humans according to what they found important in a document. The predicted summary is then compared to these human
generated summaries to check for how much important information is being retained by
the predicted summaries. But in the real world, it is difficult to find human generated
summaries for every dataset that we find because it is a tedious and cumbersome
procedure to generate these summaries. Especially in the case of Amazon reviews, since the
reviews are written in an unstructured manned it is quite difficult to generate human summaries for such a
large number of reviews.\\
Thus, in order to overcome this problem we have implemented a novel approach to
summarizing Amazon movies review dataset using a technique that we call Hierarchical
Summarization which is an extractive summarization technique, which is described in further
sections. Four different extractive summarization algorithms have been used to generate summaries from which important sentences are extracted using a feature selection technique to generate the human
generated gold summaries. Prior to all this, we classify data into positive and negative reviews using
sentiment analysis to improve the summarization of reviews and make it easier for the user to get detailed
information about a product.
The work in the report is as follows. In Section 2 we talk about the experimental design
where in we describe each step of the pipeline used to generated the gold summaries using
hierarchical summarization and the reason for using a particular technique. In Section 3 we
present our results and an analysis of the summaries by comparing it to benchmark algorithms using the
ROUGE score. In Section 4 we conclude our work and propose direction for future work.

\section{Methodology}

\subsection{Data Collection}

This work has been done on the amazon review dataset [1]. The dataset contains reviews of movies and TV shows purchased from amazon. The attributes of the dataset are reviewerID, asin, reviewerName, helpful, reviewText, overall, summary, unixReviewTime, reviewTime. Some of the important attributes include reviewerID : unique ID of the reviewer, asin : unique ID of the movie, reviewText : The review given by the reviewer corresponding to the reviewerID for the product with unique ID as asin, summary : The heading of the reviewText.

\subsection{Data Preprocessing}
In the Amazon Review Dataset, since the reviews are user generated, the data is highly unstructured. There was no general structure to punctuations, spaces etc. As a result extensive preprocessing of the data was done before further use for obtaining accurate results. We used various regular expressions and tokenizers to convert the data into a well-defined format. Keeping in mind, our aim to perform sentiment analysis and extractive summarization we selected the top ten movies based on the maximum number of reviews. On an average each selected movie had thousand reviews with average review length being two hundred fifty words each which comes to around 20 sentences each. 

\begin{figure}
  \includegraphics[width=\linewidth]{dc1.png}
  \caption{Number of Reviews for the top ten selected movies.}
  \label{fig:boat1}
\end{figure}

The plot in Fig. 1 shows the average number of reviews for the selected movie set stands at around 1000. These were the movies with highest number of reviews in the Amazon dataset that we chose for analysis.

\begin{figure}
  \includegraphics[width=\linewidth]{dc2.png}
  \caption{Mean number of sentences per review.}
  \label{fig:boat1}
\end{figure}

The graph plotted in Fig. 2  and Fig. 3 elaborates the fact that the selected movie-set had an overall 15 sentences per review per movie along with 250 words per review on an average.

\begin{figure}
  \includegraphics[width=\linewidth]{dc3.png}
  \caption{Distribution of number of words in reviews}
  \label{fig:boat1}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{dc4.png}
  \caption{Distribution of ratings of movies given on a scale of 1 to 5.}
  \label{fig:boat1}
\end{figure}

The movies have been selected in such a way that there is enough data available, comparatively, for analysis to generate out the positive and negative classes for the movies, as shown in Fig 4.

\subsection{Sentiment Analysis}\label{AA}
The reviews have been categorized on the basis of the polarity into positive and negative classes by carrying out a sentiment analysis on the pre-processed dataset. To achieve this a pre-trained Naive Bayes classifier on a movie reviews data was used which was provided by the TextBlob library. The model was trained on features like number of positive and negative words which help in determining the polarity of an input review/sentence.
\\
\\
 \phantom{x}\hspace{9ex}$P\left( A|B\right) =\dfrac {P\left( B|A\right) P\left( A\right) }{P\left( B\right) } $  
\\
\\
Here, A is the the class (positive/negative) that the review B will be classified into on the basis of the features that B contains and have been learned by the model. \\

\begin{algorithm}[H]
\KwData{Pre-processed reviews for various movies}
 \KwResult{Two lists of positive and negative pool of reviews}
 
 positive\_list $\leftarrow$ [] \;
 negative\_list $\leftarrow$ [] \;
 
 \For{each review in PreProcessedReviews}{
polarity $\leftarrow$ sentiment\_analysis(review)\;
    \eIf{polarity $>$ 0}{
        add review to positive\_list \;
    }{
        add review to negative\_list \;
    }
 }
return positive\_list, negative\_list \;
\caption{Preprocessing}
\end{algorithm}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{sentiment1.png}
  \caption{Sentiment Distribution per movie.}
  \label{fig:boat1}
\end{figure}

After doing the sentiment analysis on the pre-processed dataset, the distributions of reviews are as shown in Fig. 5. We discarded the neutral reviews, as our main focus was to concentrate on the top positive and negative aspects of the movie, and do the analysis on the same.

\subsection{Hierarchical Clustering}
The following four algorithms have been used for hierarchial clustering - 
\begin{itemize}
\item $LexRank^{[1]}$ : LexRank is used for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences.
\item $TextRank^{[2]}$ : This algorithm is a graphical text ranking algorithm in which text units best defining the tasks are added as vertices in the graph. The relation between the text units is represented as edges in the graph. After this graph based ranking algorithm is run until convergence and vertices are sorted based on their final score.
\item $LSA^{[3]}$ : Latent Semantic Analysis is an algorithm of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. It helps in reducing the dimensions by selecting the most relevant features using SVD (Singular Value Decomposition).
\item $SumBasic^{[4]}$ : This algorithm uses the probabilistic approach in which sentence weight is calculated by taking the mean of the probabilistic distribution of the words appearing in that sentence. Best scoring sentence containing the highest probability word is selected.
\end{itemize}\\
\\
\\
\\
Pseudo-Code \\

\begin{algorithm}[H]
\KwData{List of reviews}
 \KwResult{Summarization of list of reviews in 20 sentences}
 initialization\;
 final\_extractive\_summary $\leftarrow$ [ ] \;
 one\_sentence\_summary $\leftarrow$ [ ] \;
 \For{each review in ListOfReview}{
temp $\leftarrow$ review\;
    \While{numberOfSentences(temp) $>$ 1}{
    length $\leftarrow$ numberOfSentences(temp) \;
    temp $\leftarrow$ SummarizationAlgorithm(temp, length/2) \;
    }  
    add temp to one\_sentence\_summary\;
 }
 
 filtered\_summary $\leftarrow$ [ ] \;
 
 \For{each review in one\_sentence\_summary}{
    polarity $\leftarrow$ sentiment\_analysis(review) \;
    \If{$|$polarity$|$ $>$ threshold}{
        append review to filtered\_summary \;
    }
}

\While{numberOfSentences(filtered\_summary) $>$ 20}{
    length $\leftarrow$ numberOfSentences(filtered\_summary) \;
    filtered\_summary $\leftarrow$ SummarizationAlgorithm(filtered\_summary,length/2) \;
 }

return filtered\_summary \;

 \caption{Hierarchical Procedure}
\end{algorithm}

Above is our novel summarization procedure, Hierarchical Summarization that we run using each of the four above mentioned algorithms for the positive and negative collection of reviews to get four summaries of 20 sentences each which are the system generated summaries that we use for comparitive analysis later. 

The threshold for polarity was set to 0.5 for getting featured in one of the positive or negative lists. This was done to ensure that the summarization is biased to the most positive and negative reviews. 

The main motive behind using Hierarchical Summarization was to ensure the prevention of data loss that would have occured had we used each algorithm for summarization in one go. Also, it was found that summaries generated using this technique which were rich in information and thus gave dependable results for our specific dataset.\\

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{p1.png}
  \caption{Segment Polarities of Positive Summaries across different algorithms after Hierarchical Summarization}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{n1.png}
  \caption{Segment Polarities of Negative Summaries across different algorithms after Hierarchical Summarization}
  \label{fig:boat1}
\end{figure}

It can be seen from Fig 8 that applying summarization just once gives an average sentiment score of 0.2.  However, as the study is more focused upon getting the best positive and negative summaries, it is necessary to choose the best sentences at each step. Thus after applying the Hierarchical Summarization, we can see that average sentiment score for positive reviews is around 0.66 (Fig 6) while for negative reviews it is around -0.35 (Fig 7). This captures the essence of motive behind the study.

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{avg.png}
  \caption{Distribution of Sentiment Polarities over all reviews}
  \label{fig:boat1}
\end{figure}

\subsection{Feature Selection}
In order to compare the results more effectively, a benchmark was necessary which could mimic the human written summary. To achieve this, the four summaries obtained above were concatenated and a novel feature selection technique applied to obtain the 20 most important sentences for each of the positive and negative class. Some of the important features used to rank the sentences for selecting the top 20 sentences are as follows -

1. Length of Sentence – First the normalized length of the sentence is calculated. The sentence is awarded less score if its length is very small or very large. The score has been calculated by modeling the distribution on the parabolic curve. 

2. Positional Feature – If the sentence is coming at the start of the sentence or it is coming toward the end then it has been given more score. This has been done keeping in mind the increased amount of importance given to the introduction or conclusion in any piece of text. 

3. Weight of Sentence – First the TF-IDF (Term Frequency-Inverse Document Frequency) weight of all the words was calculated, and depending on the frequency of the word in the sentence, the weight of the sentence is calculated.

4. Quoted Text – If the quote exists in the sentence then the score of one is given, otherwise a score of zero is given.

5. Upper Case Text – Sentences containing more uppercase words have been given more weight. This has been done as the upper case words represent the sentiments of the users more strongly.

6. Sentiment Polarities – The absolute value of the sentiment polarity of the sentence on the scale of zero and one is taken as the score of the sentence. Absolute value is taken as the value of the negative sentiment polarity is going to be negative. As we are summarizing reviews the sentences having higher absolute polarity values are obviously better.

7. Numerical Words - The proportion of the numerical words in the total words is calculated
and accordingly the score is assigned to the sentence. More the numerical words more better is the sentence considered.

8. Density of sentence - The number of key phrases is calculated and the score assigned is directly proportional to the number of key phrases found in the sentence.

\section{Result}
\subsection{Timing Analysis}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{timing.png}
  \caption{Time (in milliseconds) Taken for Review Summarization }
  \label{fig:boat1}
\end{figure}

The above figure shows the timing profile of each of the four summarization
algorithms incorporated in our model. SumBasic, one of the most widely used
summarisation technique takes the least time while LSA takes the longest
because of the matrix calculus associated with it.

\subsection{Comparison of Summaries Using Rouge Score}

We have used ROUGE score as a metric to compare the summaries generated using the four algorithms ie textrank, lexrank, LSA and sumbasic with the gold summary generated by using the sentences ranked on the different features mentioned above. The results obtained after the comparison of positive and negative summaries with the gold summary are summarized in the tables below. Average F-Measure scores have been shown in the table as being the harmonic mean of precision and recall they represent both. Movies 1,2,3.. represent the top ten movies.

After analyzing the values it can be be concluded that LSA despite taking the longest time is giving the least accurate summary in most of the cases. Textrank turns out to be the most accurate algorithm over all the cases that is ROUGE-1 and ROUGE-2 score values for positive and negative summaries. Another observation that can be made is that the cases where the number of summaries is comparatively more (in the summarization of positive reviews of movies) lexrank and textrank are functioning better but the cases where the number of summaries is less (in the summarization of negative reviews of the movies) the sumbasic algorithm is also performing quite well. 

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{g1.png}
  \caption{ROUGE-1 Scores for positive Summaries}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{g2.png}
  \caption{ROUGE-2 Scores for positive Summaries}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{g3.png}
  \caption{ROUGE-1 Scores for Negative Summaries}
  \label{fig:boat1}
\end{figure}

So, the accuracy of algorithms is depending on the content to be summarized also.
 

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{g4.png}
  \caption{ROUGE-2 Scores for Negative Summaries}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{graph1.png}
  \caption{Graph for ROUGE-1 Score for positive Summaries with Movies \\ on x axis and 
   Average F-measure values on Y axis}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{graph2.png}
  \caption{Graph for ROUGE-2 Score for positive Summaries with Movies \\ on x axis and 
   Average F-measure values on Y axis}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{graph3.png}
  \caption{Graph for ROUGE-1 Score for negative Summaries with Movies \\ on x axis and 
   Average F-measure values on Y axis}
  \label{fig:boat1}
\end{figure}

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{graph4.png}
  \caption{Graph for ROUGE-2 Score for negative Summaries with Movies \\ on x axis and 
   Average F-measure values on Y axis}
  \label{fig:boat1}
\end{figure}



\begin{thebibliography}{00}
\bibitem{b1} G. Erkan and R. Radev ``LexRank: Graph Based lexical Centrality as Saliance in Text Summarization,'' University of Michigan, Ann Harbor, MI 48109 USA
\bibitem{b2} Rada Mihalcea and Paul Tarau , TextRank : Bringing Order into Texts, Department of Computer Science, University of North Texas.
\bibitem{b3}Thomas K Landauer and Peter W. Foltz, ``An introduction to Latent Semantic Analysis'' Discourse Processes, 25, 259-284
\bibitem{b4} Lucy Vanderwende and Hisami Suzuki, ``Beyond Sum Basic: Task focus Summarization with sentence Simplification and Lexical Expansion,'' Information Processing=g and Management, International Journal, Pages 1606-1618, Vol 43.
\bibitem{b5} DUC 2002 ``Document understanding Conference 2002'' www-nlpir.nist.gov/projects/duc/.

\end{thebibliography}



\section{Appendix}

\subsection{Names of Top 10 Movies}
The names of the top 10 movies which are selected are as follows-

1. Star Wars Trilogy THX Digitally Mastered Edition\\
2. Star Wars - Episode I, The Phantom Menace VHS \\
3. The Lord of the Rings: The Fellowship of the Ring.\\ 
4. Firefly: The Complete Series \\
5. Marvel's: The Avengers \\
6. Avatar\\
7. The Hunger Games\\
8. The Hobbit: An Unexpected Journey\\
9. Prometheus\\
10. Star Trek Into Darkness\\

These movies were selected due to the high number of their reviews so that there is more amount of data to summarize.

\subsection{Tables}

The following are the tables using which bar graphs shown above have been drawn(corresponding to Fig. 1, Fig. 2, Fig 3).\\

This table contains the movieid (asin) and and the corresponding number of reviews for the top 10 movies selected. \\

\begin{center}
 \begin{tabular}{|c | c | c|} 
 \hline
 \multicolumn{3}{|c|}{Mean No of reviews per movie} \\
 \hline
 \bfseries S.No. & \bfseries asin & \bfseries Count \\ [0.5ex] 
 \hline
 1 & 0793906091 & 949\\ 
 \hline
 2 & 630575067X & 901\\
 \hline
 3 & B00003CWT6 & 1022\\
 \hline
 4 & B0000AQS0F & 928\\
 \hline
 5 & B001KVZ6HK & 1058\\ 
 \hline
 6 & B002VPE1AW & 1004\\ 
 \hline
 7 & B003EYVXV4 & 1105\\
 \hline
 8 & B0059XTU1S & 933\\
 \hline
 9 & B005LAIHXQ & 1081\\
 \hline
 10 & B009934S5M & 1107\\[1ex] 
 \hline
\end{tabular}
\end{center}
\vspace{8mm}

This table contains the mean number of words per review per movie.\\

\begin{center}
 \begin{tabular}{|c | c | c|} 
 \hline
 \multicolumn{3}{|c|}{Mean No of words per review in movie} \\
 \hline
 \bfseries S.No. &\bfseries asin &\bfseries Avg\_Words \\ [0.5ex] 
 \hline
 1 & 0793906091 & 264.914647\\ 
 \hline
 2 & 630575067X & 264.023307\\
 \hline
 3 & B00003CWT6 & 268.896282\\
 \hline
 4 & B0000AQS0F & 181.544181\\
 \hline
 5 & B001KVZ6HK & 152.021739\\ 
 \hline
 6 & B002VPE1AW & 251.461155\\ 
 \hline
 7 & B003EYVXV4 & 160.954751\\
 \hline
 8 & B0059XTU1S & 187.561629\\
 \hline
 9 & B005LAIHXQ & 219.283996\\
 \hline
 10 & B009934S5M & 176.707317\\[1ex] 
 \hline
\end{tabular}
\end{center}
\vspace{8mm}

This table contains the mean number of sentences per review.

\begin{center}
 \begin{tabular}{|c | c | c|} 
 \hline
 \multicolumn{3}{|c|}{Mean No of sentences per review} \\
 \hline
 \bfseries S.No. &\bfseries asin &\bfseries Avg\_Sentences \\ [0.5ex] 
 \hline
 1 & 0793906091 & 17.154900\\ 
 \hline
 2 & 630575067X & 18.592675\\
 \hline
 3 & B00003CWT6 & 17.499022\\
 \hline
 4 & B0000AQS0F & 12.731681\\
 \hline
 5 & B001KVZ6HK & 11.573724\\ 
 \hline
 6 & B002VPE1AW & 16.139442\\ 
 \hline
 7 & B003EYVXV4 & 11.104977\\
 \hline
 8 & B0059XTU1S & 12.378349\\
 \hline
 9 & B005LAIHXQ & 14.414431\\
 \hline
 10 & B009934S5M & 12.335140\\[1ex] 
 \hline
\end{tabular}
\end{center}





\end{document}
